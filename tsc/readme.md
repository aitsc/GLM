# glm 复现情况
## GLM-base-官方模型(fp16)
### copa
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-copa-07-29-09-10 --task COPA --data-dir data/english_data/superglue/COPA --save data/checkpoints/pretrain/blocklm-base-blank/finetune/COPA --seq-length 256 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/blocklm-base-blank --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 0 --save-interval 10000 --log-interval 20 --eval-interval 1000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 652| overall: total = 100 accuracy = 58.0000{'epoch': 194, 'accuracy': 73.0}
### rte
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-rte-07-29-08-43 --task RTE --data-dir data/english_data/superglue/RTE --save data/checkpoints/pretrain/blocklm-base-blank/finetune/RTE --seq-length 256 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/blocklm-base-blank --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 0 --save-interval 10000 --log-interval 50 --eval-interval 10000000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 458| overall: total = 277 accuracy = 67.5090{'epoch': 264, 'accuracy': 72.20216606498195}
### boolq
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-BoolQ-07-29-10-37 --task BoolQ --data-dir data/english_data/superglue/BoolQ --save data/checkpoints/pretrain/blocklm-base-blank/finetune/BoolQ --seq-length 256 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/blocklm-base-blank --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 4 --save-interval 10000 --log-interval 50 --eval-interval 10000000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 90| overall: total = 3270 accuracy = 77.2477{'epoch': 90, 'accuracy': 77.24770642201835}
### wic
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-WiC-07-29-10-38 --task WiC --data-dir data/english_data/superglue/WiC --save data/checkpoints/pretrain/blocklm-base-blank/finetune/WiC --seq-length 256 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/blocklm-base-blank --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 1 --save-interval 10000 --log-interval 50 --eval-interval 10000000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 227| overall: total = 638 accuracy = 64.5768{'epoch': 210, 'accuracy': 66.61442006269593}
### cb
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-CB-07-29-10-40 --task CB --data-dir data/english_data/superglue/CB --save data/checkpoints/pretrain/blocklm-base-blank/finetune/CB --seq-length 256 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/blocklm-base-blank --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 3 --save-interval 10000 --log-interval 50 --eval-interval 10000000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 479| overall: total = 56 accuracy = 91.0714 f1-macro = 0.8651{'epoch': 479, 'accuracy': 91.07142857142857}
### multirc
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-MultiRC-07-29-10-40 --task MultiRC --data-dir data/english_data/superglue/MultiRC --save data/checkpoints/pretrain/blocklm-base-blank/finetune/MultiRC --seq-length 512 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/blocklm-base-blank --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 0 --save-interval 10000 --log-interval 50 --eval-interval 10000000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 436| overall: total = 4848 f1a = 0.7193 em = 0.2623 acc = 73.4530 max{'epoch': 338, 'score_dict': {'f1a': 0.7225342619636036, 'em': 0.25603357817418676, 'acc': 74.52557755775578, 'type': 'validation
', 'epoch': 338}}
### wsc_generative
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-WSC_generative-07-29-10-40 --task WSC --data-dir data/english_data/superglue/WSC --save data/checkpoints/pretrain/blocklm-base-blank/finetune/WSC --seq-length 128 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/blocklm-base-blank --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --save-interval 10000 --log-interval 50 --eval-interval 1000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 801| overall: total = 104 accuracy = 67.3077 max{'epoch': 647, 'score_dict': {'accuracy': 73.07692307692308, 'type': 'validation', 'epoch': 647}}
### wsc
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-WSC-07-29-10-40 --task WSC --data-dir data/english_data/superglue/WSC-negative --save data/checkpoints/pretrain/blocklm-base-blank/finetune/WSC --seq-length 128 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/blocklm-base-blank --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --loss-func mix --wsc-negative --length-penalty 1 --pattern-id 2 --save-interval 10000 --log-interval 50 --eval-interval 1000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 399| overall: total = 104 accuracy = 75.0000 max{'epoch': 318, 'score_dict': {'accuracy': 77.88461538461539, 'type': 'validation', 'epoch': 318}}
### record
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-ReCoRD-07-29-10-40 --task ReCoRD --data-dir data/english_data/superglue/ReCoRD --save data/checkpoints/pretrain/blocklm-base-blank/finetune/ReCoRD --seq-length 512 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/blocklm-base-blank --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 0 --save-interval 10000 --log-interval 50 --eval-interval 1000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 71| overall: total = 10000 EM = 72.0900 F1 = 72.7462 max{'epoch': 68, 'score_dict': {'EM': 72.3, 'F1': 72.96220779220786, 'type': 'validation', 'epoch': 68}}

## blocklm-blank07-23-14-42
fp16-books1-64*140000
### copa
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-COPA-07-29-13-26 --task COPA --data-dir data/english_data/superglue/COPA --save data/checkpoints/pretrain/block_base/blocklm-blank07-23-14-42/finetune/COPA --seq-length 256 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-23-14-42 --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 0 --save-interval 10000 --log-interval 20 --eval-interval 1000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 439| overall: total = 100 accuracy = 63.0000 max{'epoch': 319, 'score_dict': {'accuracy': 65.0, 'type': 'validation', 'epoch': 319}}
### rte
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-RTE-07-29-14-01 --task RTE --data-dir data/english_data/superglue/RTE --save data/checkpoints/pretrain/block_base/blocklm-blank07-23-14-42/finetune/RTE --seq-length 256 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-23-14-42 --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 0 --save-interval 10000 --log-interval 50 --eval-interval 10000000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 3069| overall: total = 277 accuracy = 52.3466 max{'epoch': 149, 'score_dict': {'accuracy': 57.40072202166065, 'type': 'validation', 'epoch': 149}}
### boolq
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-BoolQ-07-29-14-01 --task BoolQ --data-dir data/english_data/superglue/BoolQ --save data/checkpoints/pretrain/block_base/blocklm-blank07-23-14-42/finetune/BoolQ --seq-length 256 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-23-14-42 --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 4 --save-interval 10000 --log-interval 50 --eval-interval 10000000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 717| overall: total = 3270 accuracy = 68.2569 max{'epoch': 317, 'score_dict': {'accuracy': 72.32415902140673, 'type': 'validation', 'epoch': 317}}
### wic
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-WiC-07-29-14-01 --task WiC --data-dir data/english_data/superglue/WiC --save data/checkpoints/pretrain/block_base/blocklm-blank07-23-14-42/finetune/WiC --seq-length 256 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-23-14-42 --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 1 --save-interval 10000 --log-interval 50 --eval-interval 10000000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 225| overall: total = 638 accuracy = 57.8370 max{'epoch': 95, 'score_dict': {'accuracy': 61.442006269592476, 'type': 'validation', 'epoch': 95}}
### cb
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-CB-07-29-14-01 --task CB --data-dir data/english_data/superglue/CB --save data/checkpoints/pretrain/block_base/blocklm-blank07-23-14-42/finetune/CB --seq-length 256 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-23-14-42 --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 3 --save-interval 10000 --log-interval 50 --eval-interval 10000000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 701| overall: total = 56 accuracy = 76.7857 f1-macro = 0.6592 max{'epoch': 480, 'score_dict': {'accuracy': 83.92857142857143, 'f1-macro': 0.7745970341715022, 'type': 'validation', 'epoch': 480}}
### multirc
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-MultiRC-07-29-14-02 --task MultiRC --data-dir data/english_data/superglue/MultiRC --save data/checkpoints/pretrain/block_base/blocklm-blank07-23-14-42/finetune/MultiRC --seq-length 512 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-23-14-42 --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 0 --save-interval 10000 --log-interval 50 --eval-interval 10000000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
### wsc_generative
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-WSC_generative-07-29-14-02 --task WSC --data-dir data/english_data/superglue/WSC --save data/checkpoints/pretrain/block_base/blocklm-blank07-23-14-42/finetune/WSC --seq-length 128 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-23-14-42 --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --save-interval 10000 --log-interval 50 --eval-interval 1000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 942| overall: total = 104 accuracy = 58.6538 max{'epoch': 307, 'score_dict': {'accuracy': 69.23076923076923, 'type': 'validation', 'epoch': 307}}
### wsc
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-WSC-07-30-05-08 --task WSC --data-dir data/english_data/superglue/WSC-negative --save data/checkpoints/pretrain/block_base/blocklm-blank07-23-14-42/finetune/WSC --seq-length 128 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-23-14-42 --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --loss-func mix --wsc-negative --length-penalty 1 --pattern-id 2 --save-interval 10000 --log-interval 50 --eval-interval 1000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 428| overall: total = 104 accuracy = 62.5000 max{'epoch': 369, 'score_dict': {'accuracy': 66.34615384615384, 'type': 'validation', 'epoch': 369}}
### record
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-ReCoRD-07-29-14-02 --task ReCoRD --data-dir data/english_data/superglue/ReCoRD --save data/checkpoints/pretrain/block_base/blocklm-blank07-23-14-42/finetune/ReCoRD --seq-length 512 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-23-14-42 --fp16 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 0 --save-interval 10000 --log-interval 50 --eval-interval 1000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0

## blocklm-blank07-24-09-23
fp32-books1-64*140000
### copa
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-COPA-07-29-14-53 --task COPA --data-dir data/english_data/superglue/COPA --save data/checkpoints/pretrain/block_base/blocklm-blank07-24-09-23/finetune/COPA --seq-length 256 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-24-09-23 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 0 --save-interval 10000 --log-interval 20 --eval-interval 1000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --fp32-allreduce --num-workers 0
    - |epoch: 729| overall: total = 100 accuracy = 62.0000 max{'epoch': 284, 'score_dict': {'accuracy': 68.0, 'type': 'validation', 'epoch': 284}}
### rte
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-RTE-07-29-14-53 --task RTE --data-dir data/english_data/superglue/RTE --save data/checkpoints/pretrain/block_base/blocklm-blank07-24-09-23/finetune/RTE --seq-length 256 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-24-09-23 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 0 --save-interval 10000 --log-interval 50 --eval-interval 10000000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --fp32-allreduce --num-workers 0
    - |epoch: 2643| overall: total = 277 accuracy = 51.9856 max{'epoch': 383, 'score_dict': {'accuracy': 61.371841155234655, 'type': 'validation', 'epoch': 383}}
### boolq
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-BoolQ-07-30-04-43 --task BoolQ --data-dir data/english_data/superglue/BoolQ --save data/checkpoints/pretrain/block_base/blocklm-blank07-24-09-23/finetune/BoolQ --seq-length 256 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-24-09-23 --fp32-allreduce --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 4 --save-interval 10000 --log-interval 50 --eval-interval 10000000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 662| overall: total = 3270 accuracy = 69.6330 max{'epoch': 286, 'score_dict': {'accuracy': 73.18042813455658, 'type': 'validation', 'epoch': 286}}
### wic
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-WiC-07-30-04-44 --task WiC --data-dir data/english_data/superglue/WiC --save data/checkpoints/pretrain/block_base/blocklm-blank07-24-09-23/finetune/WiC --seq-length 256 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-24-09-23 --fp32-allreduce --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 1 --save-interval 10000 --log-interval 50 --eval-interval 10000000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 201| overall: total = 638 accuracy = 57.5235 max{'epoch': 63, 'score_dict': {'accuracy': 59.24764890282132, 'type': 'validation', 'epoch': 63}}
### cb
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-CB-07-30-04-44 --task CB --data-dir data/english_data/superglue/CB --save data/checkpoints/pretrain/block_base/blocklm-blank07-24-09-23/finetune/CB --seq-length 256 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-24-09-23 --fp32-allreduce --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 3 --save-interval 10000 --log-interval 50 --eval-interval 10000000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 635| overall: total = 56 accuracy = 83.9286 f1-macro = 0.7739 max{'epoch': 361, 'score_dict': {'accuracy': 83.92857142857143, 'f1-macro': 0.7899822018815152, 'type': 'validation', 'epoch': 361}}
### multirc
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-MultiRC-07-30-04-44 --task MultiRC --data-dir data/english_data/superglue/MultiRC --save data/checkpoints/pretrain/block_base/blocklm-blank07-24-09-23/finetune/MultiRC --seq-length 512 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-24-09-23 --fp32-allreduce --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 0 --save-interval 10000 --log-interval 50 --eval-interval 10000000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
### wsc_generative
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-WSC_generative-07-30-04-44 --task WSC --data-dir data/english_data/superglue/WSC --save data/checkpoints/pretrain/block_base/blocklm-blank07-24-09-23/finetune/WSC --seq-length 128 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-24-09-23 --fp32-allreduce --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --save-interval 10000 --log-interval 50 --eval-interval 1000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 941| overall: total = 104 accuracy = 65.3846 max{'epoch': 546, 'score_dict': {'accuracy': 67.3076923076923, 'type': 'validation', 'epoch': 546}}
### wsc
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-WSC-07-30-05-09 --task WSC --data-dir data/english_data/superglue/WSC-negative --save data/checkpoints/pretrain/block_base/blocklm-blank07-24-09-23/finetune/WSC --seq-length 128 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-24-09-23 --fp32-allreduce --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --loss-func mix --wsc-negative --length-penalty 1 --pattern-id 2 --save-interval 10000 --log-interval 50 --eval-interval 1000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 432| overall: total = 104 accuracy = 59.6154 max{'epoch': 359, 'score_dict': {'accuracy': 63.46153846153846, 'type': 'validation', 'epoch': 359}}
### record
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-ReCoRD-07-30-04-44 --task ReCoRD --data-dir data/english_data/superglue/ReCoRD --save data/checkpoints/pretrain/block_base/blocklm-blank07-24-09-23/finetune/ReCoRD --seq-length 512 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-24-09-23 --fp32-allreduce --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 0 --save-interval 10000 --log-interval 50 --eval-interval 1000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0

## blocklm-blank07-25-13-30
fp32-wiki+books1-64*390000
### copa
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-COPA-07-29-14-54 --task COPA --data-dir data/english_data/superglue/COPA --save data/checkpoints/pretrain/block_base/blocklm-blank07-25-13-30/finetune/COPA --seq-length 256 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-25-13-30 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 0 --save-interval 10000 --log-interval 20 --eval-interval 1000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --fp32-allreduce --num-workers 0
    - |epoch: 728| overall: total = 100 accuracy = 63.0000 max{'epoch': 292, 'score_dict': {'accuracy': 68.0, 'type': 'validation', 'epoch': 292}}
### rte
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-RTE-07-29-14-54 --task RTE --data-dir data/english_data/superglue/RTE --save data/checkpoints/pretrain/block_base/blocklm-blank07-25-13-30/finetune/RTE --seq-length 256 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-25-13-30 --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 0 --save-interval 10000 --log-interval 50 --eval-interval 10000000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --fp32-allreduce --num-workers 0
    - |epoch: 2652| overall: total = 277 accuracy = 53.7906 max{'epoch': 120, 'score_dict': {'accuracy': 63.537906137184116, 'type': 'validation', 'epoch': 120}}
### boolq
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-BoolQ-07-30-04-45 --task BoolQ --data-dir data/english_data/superglue/BoolQ --save data/checkpoints/pretrain/block_base/blocklm-blank07-25-13-30/finetune/BoolQ --seq-length 256 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-25-13-30 --fp32-allreduce --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 4 --save-interval 10000 --log-interval 50 --eval-interval 10000000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 663| overall: total = 3270 accuracy = 64.1284 max{'epoch': 189, 'score_dict': {'accuracy': 74.40366972477064, 'type': 'validation', 'epoch': 189}}
### wic
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-WiC-07-30-04-45 --task WiC --data-dir data/english_data/superglue/WiC --save data/checkpoints/pretrain/block_base/blocklm-blank07-25-13-30/finetune/WiC --seq-length 256 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-25-13-30 --fp32-allreduce --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 1 --save-interval 10000 --log-interval 50 --eval-interval 10000000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 208| overall: total = 638 accuracy = 62.0690 max{'epoch': 168, 'score_dict': {'accuracy': 63.63636363636363, 'type': 'validation', 'epoch': 168}}
### cb
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-CB-07-30-04-45 --task CB --data-dir data/english_data/superglue/CB --save data/checkpoints/pretrain/block_base/blocklm-blank07-25-13-30/finetune/CB --seq-length 256 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-25-13-30 --fp32-allreduce --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 3 --save-interval 10000 --log-interval 50 --eval-interval 10000000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 668| overall: total = 56 accuracy = 82.1429 f1-macro = 0.7839 max{'epoch': 335, 'score_dict': {'accuracy': 87.5, 'f1-macro': 0.8533960292580983, 'type': 'validation', 'epoch': 335}}
### multirc
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-MultiRC-07-30-04-45 --task MultiRC --data-dir data/english_data/superglue/MultiRC --save data/checkpoints/pretrain/block_base/blocklm-blank07-25-13-30/finetune/MultiRC --seq-length 512 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-25-13-30 --fp32-allreduce --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 0 --save-interval 10000 --log-interval 50 --eval-interval 10000000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
### wsc_generative
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-WSC_generative-07-30-04-45 --task WSC --data-dir data/english_data/superglue/WSC --save data/checkpoints/pretrain/block_base/blocklm-blank07-25-13-30/finetune/WSC --seq-length 128 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-25-13-30 --fp32-allreduce --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --save-interval 10000 --log-interval 50 --eval-interval 1000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 922| overall: total = 104 accuracy = 73.0769 max{'epoch': 296, 'score_dict': {'accuracy': 76.92307692307692, 'type': 'validation', 'epoch': 296}}
### wsc
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-WSC-07-30-05-09 --task WSC --data-dir data/english_data/superglue/WSC-negative --save data/checkpoints/pretrain/block_base/blocklm-blank07-25-13-30/finetune/WSC --seq-length 128 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-25-13-30 --fp32-allreduce --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --loss-func mix --wsc-negative --length-penalty 1 --pattern-id 2 --save-interval 10000 --log-interval 50 --eval-interval 1000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0
    - |epoch: 433| overall: total = 104 accuracy = 56.7308 max{'epoch': 159, 'score_dict': {'accuracy': 66.34615384615384, 'type': 'validation', 'epoch': 159}}
### record
- python -u finetune_glm.py --finetune --cloze-eval --experiment-name blank-base-ReCoRD-07-30-04-45 --task ReCoRD --data-dir data/english_data/superglue/ReCoRD --save data/checkpoints/pretrain/block_base/blocklm-blank07-25-13-30/finetune/ReCoRD --seq-length 512 --checkpoint-activations --eval-batch-size 16 --save-epoch 100000 --block-lm --num-layers 12 --hidden-size 768 --num-attention-heads 12 --max-position-embeddings 512 --tokenizer-model-type bert-base-uncased --tokenizer-type BertWordPieceTokenizer --load-pretrained data/checkpoints/pretrain/block_base/blocklm-blank07-25-13-30 --fp32-allreduce --lr-decay-style linear --warmup 0.1 --weight-decay 1.0e-1 --pattern-id 0 --save-interval 10000 --log-interval 50 --eval-interval 1000 --eval-iters 100 --batch-size 16 --epochs 10000 --lr 1e-5 --overwrite --num-workers 0